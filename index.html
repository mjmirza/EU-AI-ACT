<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.17.0/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.17.0/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.17.0/dist/index.js"></script><script>(()=>{setTimeout(()=>{const{markmap:O,mm:h}=window,M=new O.Toolbar;M.attach(h);const re=M.render();re.setAttribute("style","position:absolute;bottom:20px;right:20px"),document.body.append(re)})})()</script><script>((i,L,f,o)=>{const w=i();window.mm=w.Markmap.create("svg#mindmap",(L||w.deriveOptions)(o),f)})(()=>window.markmap,null,{"content":"EU AI Act 2024","children":[{"content":"Introduction","children":[{"content":"Purpose","children":[{"content":"<strong>Protect Fundamental Rights</strong>: Ensure AI systems are developed and used in a way that respects European values and fundamental rights including privacy, non-discrimination, and consumer protection.","children":[],"payload":{"lines":"4,5"}},{"content":"<strong>Safety and Liability</strong>: Address the risks associated with specific uses of AI and provide a legal framework for determining liabilities.","children":[],"payload":{"lines":"5,6"}},{"content":"<strong>Foster Innovation</strong>: Create an environment that encourages investment in and development of ethical, secure, and trustworthy AI across the EU.","children":[],"payload":{"lines":"6,8"}}],"payload":{"lines":"3,4"}},{"content":"Scope","children":[{"content":"<strong>Geographical Reach</strong>: Applies to AI systems deployed within the EU, regardless of where the provider is based. It also applies to providers and users located outside the EU if the output of the AI system affects individuals within the EU.","children":[],"payload":{"lines":"9,10"}},{"content":"<strong>Sectoral Coverage</strong>: Covers a wide range of sectors including but not limited to healthcare, transportation, policing, and employment.","children":[],"payload":{"lines":"10,11"}},{"content":"<strong>Exclusions</strong>: Certain uses of AI, such as those involving national security, are exempt from the scope of the act.","children":[],"payload":{"lines":"11,13"}}],"payload":{"lines":"8,9"}},{"content":"Definitions","children":[{"content":"<strong>Artificial Intelligence System (AI System)</strong>: Software that is developed with one or more techniques and approaches capable of generating outputs such as content, predictions, recommendations, or decisions influencing the environments they interact with.","children":[],"payload":{"lines":"14,15"}},{"content":"<strong>High-Risk AI System</strong>: AI systems identified as posing significant risks to the health, safety, or fundamental rights of individuals. These systems are subject to stricter regulatory requirements.","children":[],"payload":{"lines":"15,16"}},{"content":"<strong>Real-Time Remote Biometric Identification System</strong>: A system that processes biometric data in real time, collected from remote or publicly accessible spaces, to identify a specific individual.","children":[],"payload":{"lines":"16,17"}},{"content":"<strong>AI System Provider</strong>: The natural or legal person, public authority, agency, or other body that develops an AI system or has it developed, to be placed on the market or put into service under its own name or trademark.","children":[],"payload":{"lines":"17,18"}},{"content":"<strong>User of an AI System</strong>: A natural or legal person, public authority, agency, or other body that uses an AI system under its authority, except where the AI system is used in the course of a personal non-professional activity.","children":[],"payload":{"lines":"18,20"}}],"payload":{"lines":"13,14"}}],"payload":{"lines":"2,3"}},{"content":"Classification of AI Systems","children":[{"content":"Risk-based Approach","children":[{"content":"Minimal/No Risk","children":[{"content":"<strong>Description</strong>: AI systems that pose little to no risk to the rights or safety of individuals. The vast majority of AI applications fall into this category.","children":[],"payload":{"lines":"25,26"}},{"content":"<strong>Examples</strong>: AI-driven video games, spam filters, and some types of AI-based content recommendation.","children":[],"payload":{"lines":"26,27"}},{"content":"<strong>Regulation</strong>: Minimal regulatory intervention. Transparency or the provision of information to users may be encouraged but not mandated.","children":[],"payload":{"lines":"27,29"}}],"payload":{"lines":"24,25"}},{"content":"Limited Risk","children":[{"content":"<strong>Description</strong>: AI systems where there is a slight risk that could affect individuals' rights or safety, requiring transparency measures.","children":[],"payload":{"lines":"30,31"}},{"content":"<strong>Examples</strong>: Chatbots that provide information to consumers. Users should be informed that they are interacting with an AI.","children":[],"payload":{"lines":"31,32"}},{"content":"<strong>Regulation</strong>: Requirements for transparency. For instance, it should be clear to users that they are interacting with an AI system, allowing them to make informed decisions about their engagement.","children":[],"payload":{"lines":"32,34"}}],"payload":{"lines":"29,30"}},{"content":"High Risk","children":[{"content":"<strong>Description</strong>: AI systems that pose significant risks to health, safety, or fundamental rights. Such systems are subject to stringent regulatory requirements before they can be put on the market.","children":[],"payload":{"lines":"35,36"}},{"content":"<strong>Examples</strong>: Recruitment AI tools, AI used in critical infrastructure, educational or vocational training software that may determine access to education and professional course of individuals, and AI systems involved in law enforcement.","children":[],"payload":{"lines":"36,37"}},{"content":"<strong>Regulation</strong>: Mandatory compliance with specific requirements such as data and record-keeping, transparency, and provision of information to users, robustness, accuracy, and security measures. Pre-market conformity assessments may also be required.","children":[],"payload":{"lines":"37,39"}}],"payload":{"lines":"34,35"}},{"content":"Unacceptable Risk","children":[{"content":"<strong>Description</strong>: AI applications that pose a clear threat to the safety, livelihoods, and rights of people, deemed unacceptable and prohibited from being developed or used.","children":[],"payload":{"lines":"40,41"}},{"content":"<strong>Examples</strong>: Social scoring systems by governments that judge individuals based on their behavior. Real-time biometric identification systems in publicly accessible spaces for law enforcement purposes, with certain narrow exceptions.","children":[],"payload":{"lines":"41,42"}},{"content":"<strong>Regulation</strong>: These systems are banned outright due to the severe risks they present.","children":[],"payload":{"lines":"42,44"}}],"payload":{"lines":"39,40"}}],"payload":{"lines":"21,22"}}],"payload":{"lines":"20,21"}},{"content":"Requirements for High-Risk AI Systems","children":[{"content":"Data Governance","children":[{"content":"<strong>Purpose</strong>: To ensure the quality and integrity of data used by high-risk AI systems throughout their lifecycle.","children":[],"payload":{"lines":"47,48"}},{"content":"<strong>Requirements</strong>:","children":[{"content":"Data sets used must be relevant, representative, free of errors, and complete.","children":[],"payload":{"lines":"49,50"}},{"content":"Procedures should be in place to handle inaccuracies in data.","children":[],"payload":{"lines":"50,51"}},{"content":"Measures to ensure personal data is processed in accordance with GDPR.","children":[],"payload":{"lines":"51,53"}}],"payload":{"lines":"48,53"}}],"payload":{"lines":"46,47"}},{"content":"Documentation and Record-keeping","children":[{"content":"<strong>Purpose</strong>: To maintain a detailed record of the AI system's development process, enabling transparency and accountability.","children":[],"payload":{"lines":"54,55"}},{"content":"<strong>Requirements</strong>:","children":[{"content":"Documentation should cover the dataset used for training, development, and testing, including the methodology, parameters, and any data preprocessing techniques.","children":[],"payload":{"lines":"56,57"}},{"content":"Records of all operational instances should be kept to enable traceability of the AI system's decisions.","children":[],"payload":{"lines":"57,59"}}],"payload":{"lines":"55,59"}}],"payload":{"lines":"53,54"}},{"content":"Transparency and Provision of Information","children":[{"content":"<strong>Purpose</strong>: To ensure that users are fully informed about the AI system's capabilities, limitations, and intended use.","children":[],"payload":{"lines":"60,61"}},{"content":"<strong>Requirements</strong>:","children":[{"content":"Clear, accessible information about the AI system's purpose, capabilities, and decision-making processes.","children":[],"payload":{"lines":"62,63"}},{"content":"Information on the level of human oversight and how to interpret the AI system's output.","children":[],"payload":{"lines":"63,65"}}],"payload":{"lines":"61,65"}}],"payload":{"lines":"59,60"}},{"content":"Human Oversight","children":[{"content":"<strong>Purpose</strong>: To ensure that human judgment plays a critical role in the operation of high-risk AI systems, safeguarding against unintended consequences.","children":[],"payload":{"lines":"66,67"}},{"content":"<strong>Requirements</strong>:","children":[{"content":"Mechanisms for human oversight must be integrated into the AI system's deployment, allowing for human intervention and the ability to override decisions made by the AI.","children":[],"payload":{"lines":"68,69"}},{"content":"Training for operators to understand and effectively manage the AI system.","children":[],"payload":{"lines":"69,71"}}],"payload":{"lines":"67,71"}}],"payload":{"lines":"65,66"}},{"content":"Robustness, Security, and Accuracy","children":[{"content":"<strong>Purpose</strong>: To ensure that high-risk AI systems operate reliably, securely, and accurately under all conditions.","children":[],"payload":{"lines":"72,73"}},{"content":"<strong>Requirements</strong>:","children":[{"content":"AI systems must be resilient to errors, inconsistencies, and attempts to exploit system vulnerabilities.","children":[],"payload":{"lines":"74,75"}},{"content":"Regular testing for robustness and accuracy, especially in response to evolving external threats.","children":[],"payload":{"lines":"75,76"}},{"content":"Implementation of cybersecurity measures to protect against unauthorized access and malicious use.","children":[],"payload":{"lines":"76,78"}}],"payload":{"lines":"73,78"}}],"payload":{"lines":"71,72"}}],"payload":{"lines":"44,45"}},{"content":"Governance and Enforcement","children":[{"content":"National Supervisory Authorities","children":[{"content":"<strong>Role</strong>: Each EU member state is required to designate one or more national supervisory authorities (NSAs) responsible for overseeing the implementation of the AI Act within their jurisdiction.","children":[],"payload":{"lines":"81,82"}},{"content":"<strong>Responsibilities</strong>:","children":[{"content":"Monitoring and enforcement of the AI Act's requirements for AI systems placed on their market or used within their territory.","children":[],"payload":{"lines":"83,84"}},{"content":"Conducting investigations into non-compliance, including audits of AI systems and practices.","children":[],"payload":{"lines":"84,85"}},{"content":"Issuing warnings, orders, and, if necessary, imposing fines or other penalties on entities failing to comply with the Act.","children":[],"payload":{"lines":"85,86"}},{"content":"Providing guidance and assistance to AI system providers and users to help them understand and fulfill their obligations under the Act.","children":[],"payload":{"lines":"86,88"}}],"payload":{"lines":"82,88"}}],"payload":{"lines":"80,81"}},{"content":"European Artificial Intelligence Board","children":[{"content":"<strong>Role</strong>: The European Artificial Intelligence Board (EAIB) is established to ensure a consistent application of the AI Act across the EU and to facilitate cooperation among the NSAs.","children":[],"payload":{"lines":"89,90"}},{"content":"<strong>Composition</strong>: The Board is composed of representatives from each of the NSAs and the European Commission.","children":[],"payload":{"lines":"90,91"}},{"content":"<strong>Responsibilities</strong>:","children":[{"content":"Advising the European Commission on matters related to the AI Act, including updates and amendments to the Act.","children":[],"payload":{"lines":"92,93"}},{"content":"Facilitating the exchange of best practices and information between national supervisory authorities.","children":[],"payload":{"lines":"93,94"}},{"content":"Preparing and publishing guidelines on various aspects of the AI Act to assist with its interpretation and application.","children":[],"payload":{"lines":"94,95"}},{"content":"Reviewing the list of high-risk AI systems and recommending changes or updates to ensure it remains relevant and comprehensive.","children":[],"payload":{"lines":"95,97"}}],"payload":{"lines":"91,97"}}],"payload":{"lines":"88,89"}}],"payload":{"lines":"78,79"}},{"content":"Legal Obligations","children":[{"content":"For Providers","children":[{"content":"Conformity Assessments","children":[{"content":"<strong>Purpose</strong>: To verify that AI systems comply with the requirements set out in the EU AI Act before they are placed on the market or put into service.","children":[],"payload":{"lines":"103,104"}},{"content":"<strong>Process</strong>: Involves a detailed review of the AI system's design, development, and deployment processes to ensure they meet specific standards related to data governance, transparency, and human oversight among others.","children":[],"payload":{"lines":"104,105"}},{"content":"<strong>Documentation</strong>: Providers must compile and maintain a technical file detailing the system's compliance with the necessary requirements, which must be made available to national supervisory authorities upon request.","children":[],"payload":{"lines":"105,107"}}],"payload":{"lines":"102,103"}},{"content":"CE Markings","children":[{"content":"<strong>Significance</strong>: The CE marking is a certification mark that indicates conformity with health, safety, and environmental protection standards for products sold within the European Economic Area (EEA).","children":[],"payload":{"lines":"108,109"}},{"content":"<strong>Requirement</strong>: High-risk AI systems must bear the CE marking to demonstrate they have undergone the necessary conformity assessment and are deemed to meet EU safety, health, and environmental protection requirements.","children":[],"payload":{"lines":"109,110"}},{"content":"<strong>Procedure</strong>: Once an AI system has successfully passed the conformity assessment, the provider affixes the CE mark to it, indicating its compliance and allowing it to be marketed and used throughout the EU.","children":[],"payload":{"lines":"110,112"}}],"payload":{"lines":"107,108"}}],"payload":{"lines":"99,100"}},{"content":"For Users","children":[{"content":"Appropriate Use","children":[{"content":"<strong>Expectation</strong>: Users must operate the AI system in accordance with the provider's instructions, including any recommended conditions or limitations of use.","children":[],"payload":{"lines":"116,117"}},{"content":"<strong>Responsibility</strong>: Users should not misuse the AI system in a way that could pose risks to people's rights or safety. They must be aware of the system's intended purpose and adhere to it strictly.","children":[],"payload":{"lines":"117,119"}}],"payload":{"lines":"115,116"}},{"content":"Monitoring","children":[{"content":"<strong>Obligation</strong>: Users must actively monitor the performance of high-risk AI systems to detect and report any malfunctions or deviations from expected performance standards.","children":[],"payload":{"lines":"120,121"}},{"content":"<strong>Action</strong>: In cases where issues are identified, users are expected to take immediate corrective action, which may include ceasing the use of the system and notifying the provider and relevant authorities if necessary.","children":[],"payload":{"lines":"121,123"}}],"payload":{"lines":"119,120"}}],"payload":{"lines":"112,113"}}],"payload":{"lines":"97,98"}},{"content":"Market Surveillance","children":[{"content":"Identification of Non-Compliant AI Systems","children":[{"content":"<strong>Objective</strong>: To detect AI systems that do not comply with the EU AI Act's requirements, posing potential risks to users' rights and safety.","children":[],"payload":{"lines":"126,127"}},{"content":"<strong>Methods</strong>:","children":[{"content":"Regular and systematic monitoring of the market by national supervisory authorities.","children":[],"payload":{"lines":"128,129"}},{"content":"Investigations triggered by complaints or reports from consumers, other businesses, or organizations.","children":[],"payload":{"lines":"129,130"}},{"content":"Analysis of AI systems based on risk profiles, including random checks and targeted assessments of systems suspected of non-compliance.","children":[],"payload":{"lines":"130,131"}}],"payload":{"lines":"127,131"}},{"content":"<strong>Tools</strong>: Use of digital tools and cooperation with other EU member states to share information about potentially non-compliant AI systems.","children":[],"payload":{"lines":"131,133"}}],"payload":{"lines":"125,126"}},{"content":"Corrective Actions","children":[{"content":"<strong>Purpose</strong>: To remedy any identified non-compliance and mitigate the risks posed by non-compliant AI systems.","children":[],"payload":{"lines":"134,135"}},{"content":"<strong>Measures</strong>:","children":[{"content":"<strong>For Minor Non-compliance</strong>: Providers may be given a deadline to rectify the issues. This could involve modifying the AI system, updating documentation, or improving user guidance.","children":[],"payload":{"lines":"136,137"}},{"content":"<strong>For Serious Risks</strong>: Immediate actions can be demanded, such as withdrawing the AI system from the market, issuing a recall, or imposing temporary restrictions on its use.","children":[],"payload":{"lines":"137,138"}},{"content":"<strong>Penalties</strong>: For significant violations or failure to take corrective action, financial penalties may be imposed, and criminal charges could be considered in severe cases.","children":[],"payload":{"lines":"138,139"}}],"payload":{"lines":"135,139"}},{"content":"<strong>Transparency</strong>: Authorities must inform the public about the non-compliance of high-risk AI systems, especially if there is an immediate risk to health or safety.","children":[],"payload":{"lines":"139,140"}},{"content":"<strong>Cross-border Enforcement</strong>: Given the digital and cross-border nature of AI, national supervisory authorities will cooperate with each other and the European Commission to ensure effective enforcement across the EU.","children":[],"payload":{"lines":"140,142"}}],"payload":{"lines":"133,134"}}],"payload":{"lines":"123,124"}},{"content":"Prohibited AI Practices","children":[{"content":"Real-time Biometric Identification Systems in Public Spaces (with exceptions)","children":[{"content":"<strong>Description</strong>: The use of AI systems for the real-time identification of individuals in public spaces using biometric data (e.g., facial recognition) is generally prohibited.","children":[],"payload":{"lines":"145,146"}},{"content":"<strong>Exceptions</strong>: Limited exceptions exist for specific law enforcement purposes, such as the prevention of a specific, substantial and imminent threat to public security. These exceptions are subject to stringent checks and balances, including judicial or independent authorization, and notification to the public.","children":[],"payload":{"lines":"146,148"}}],"payload":{"lines":"144,145"}},{"content":"AI Systems That Manipulate Humans","children":[{"content":"<strong>Description</strong>: AI systems designed or used in a way that manipulates individuals through subliminal techniques or takes advantage of vulnerabilities (due to age, physical or mental disability) to materially distort a person's behavior in a manner that causes or is likely to cause that individual harm.","children":[],"payload":{"lines":"149,150"}},{"content":"<strong>Examples</strong>: An AI application that targets children or individuals with disabilities to influence their behavior in harmful ways, such as encouraging dangerous challenges or habits.","children":[],"payload":{"lines":"150,152"}}],"payload":{"lines":"148,149"}},{"content":"AI Systems That Exploit Vulnerable Groups","children":[{"content":"<strong>Description</strong>: Similar to manipulation, this category specifically prohibits AI practices that target and exploit the vulnerabilities of specific groups, leading to harm or adverse outcomes.","children":[],"payload":{"lines":"153,154"}},{"content":"<strong>Consideration</strong>: This prohibition is aimed at protecting groups that may be more susceptible to harm or coercion, ensuring that AI technologies do not exacerbate inequalities or lead to exploitation.","children":[],"payload":{"lines":"154,156"}}],"payload":{"lines":"152,153"}},{"content":"Social Scoring Systems","children":[{"content":"<strong>Description</strong>: The creation or use of AI systems by public authorities (or on their behalf) to evaluate the trustworthiness of individuals over time based on their social behavior or predicted personal or social characteristics is banned.","children":[],"payload":{"lines":"157,158"}},{"content":"<strong>Implications</strong>: Such systems, often associated with mass surveillance and control, are prohibited due to their potential to infringe on individual freedoms and rights, discriminate against individuals or groups, and limit personal autonomy.","children":[],"payload":{"lines":"158,160"}}],"payload":{"lines":"156,157"}}],"payload":{"lines":"142,143"}},{"content":"Exemptions and Special Cases","children":[{"content":"Research and Development","children":[{"content":"<strong>Scope of Exemption</strong>: AI systems developed or used exclusively for research and development purposes are exempt from certain obligations under the Act.","children":[],"payload":{"lines":"163,164"}},{"content":"<strong>Purpose</strong>: To encourage innovation and the advancement of AI technology without the constraints of full regulatory compliance, provided these activities do not lead to commercial applications directly or have any immediate impact on individuals or society.","children":[],"payload":{"lines":"164,165"}},{"content":"<strong>Conditions</strong>:","children":[{"content":"The exemption is limited to AI systems that are not placed on the market or put into service outside of the R&amp;D context.","children":[],"payload":{"lines":"166,167"}},{"content":"Researchers and developers must still ensure basic ethical standards and respect fundamental rights during the development process.","children":[],"payload":{"lines":"167,169"}}],"payload":{"lines":"165,169"}}],"payload":{"lines":"162,163"}},{"content":"Military Use","children":[{"content":"<strong>Scope of Exemption</strong>: AI systems intended for use in military applications are generally exempt from the provisions of the EU AI Act.","children":[],"payload":{"lines":"170,171"}},{"content":"<strong>Rationale</strong>: National security considerations and the specific regulatory frameworks that govern military technology and operations.","children":[],"payload":{"lines":"171,172"}},{"content":"<strong>Implications</strong>:","children":[{"content":"This exemption acknowledges the unique requirements and governance structures of defense and national security sectors.","children":[],"payload":{"lines":"173,174"}},{"content":"Despite the exemption under the EU AI Act, military AI applications are still subject to other forms of regulation at both national and international levels, including laws governing warfare, humanitarian law, and specific defense-related regulatory frameworks.","children":[],"payload":{"lines":"174,176"}}],"payload":{"lines":"172,176"}}],"payload":{"lines":"169,170"}}],"payload":{"lines":"160,161"}},{"content":"Implementation and Timeline","children":[{"content":"Transitional Periods","children":[{"content":"<strong>Purpose</strong>: Transitional periods are designed to give entities subject to the AI Act—whether they are providers, users, or regulators—sufficient time to adapt to the new regulatory environment.","children":[],"payload":{"lines":"179,180"}},{"content":"<strong>Duration</strong>:","children":[{"content":"The Act specifies a period following its adoption and entry into force during which its provisions do not yet apply. This allows time for necessary adjustments and preparations.","children":[],"payload":{"lines":"181,182"}},{"content":"For high-risk AI systems already in use or on the market, there may be additional transitional provisions allowing for a phased compliance with the most stringent requirements.","children":[],"payload":{"lines":"182,183"}}],"payload":{"lines":"180,183"}},{"content":"<strong>Implications</strong>: These periods are critical for ensuring that the introduction of the AI Act does not disrupt existing operations or innovation trajectories unduly, providing a smoother path to compliance.","children":[],"payload":{"lines":"183,185"}}],"payload":{"lines":"178,179"}},{"content":"Review and Update Mechanisms","children":[{"content":"<strong>Adaptability</strong>: Given the fast pace of technological development in AI, the Act includes mechanisms to regularly review and update its provisions to stay relevant and effective.","children":[],"payload":{"lines":"186,187"}},{"content":"<strong>Process</strong>:","children":[{"content":"The European Commission, in collaboration with the European Artificial Intelligence Board and other stakeholders, is tasked with conducting periodic reviews of the Act's effectiveness and its alignment with technological advancements and societal needs.","children":[],"payload":{"lines":"188,189"}},{"content":"These reviews can lead to amendments to the Act or the introduction of new guidance for its implementation, ensuring that the regulatory framework remains fit for purpose.","children":[],"payload":{"lines":"189,190"}}],"payload":{"lines":"187,190"}},{"content":"<strong>Considerations</strong>: Special attention is given to the evolution of AI technologies, the emergence of new risks, and the development of international standards in AI. This forward-looking approach is meant to ensure that the EU's regulatory environment not only protects citizens but also supports innovation.","children":[],"payload":{"lines":"190,192"}}],"payload":{"lines":"185,186"}}],"payload":{"lines":"176,177"}},{"content":"International Implications","children":[{"content":"Impact on Global AI Standards","children":[{"content":"<strong>Setting a Precedent</strong>: As one of the first comprehensive legislative frameworks for AI, the EU AI Act is likely to set a benchmark for AI regulation globally. Other countries and international bodies may look to the Act as a model when developing or updating their own AI regulations.","children":[],"payload":{"lines":"195,196"}},{"content":"<strong>Raising the Bar</strong>: By establishing high standards for transparency, safety, and accountability, the Act encourages the adoption of similar principles worldwide, potentially leading to a global baseline for ethical AI development and deployment.","children":[],"payload":{"lines":"196,197"}},{"content":"<strong>Collaboration and Convergence</strong>: The EU's approach could foster greater international collaboration on AI governance, facilitating the development of common standards and interoperability between different regulatory regimes.","children":[],"payload":{"lines":"197,199"}}],"payload":{"lines":"194,195"}},{"content":"Alignment with Other Jurisdictions","children":[{"content":"<strong>Harmonization Efforts</strong>: The EU is actively seeking to align its AI regulations with those of other jurisdictions to reduce barriers to international trade and innovation. This includes dialogues and agreements on mutual recognition of AI standards and certifications.","children":[],"payload":{"lines":"200,201"}},{"content":"<strong>Challenges and Opportunities</strong>: While the aim is to create a coherent international framework, differences in cultural values, legal traditions, and strategic priorities can pose challenges to alignment. However, these differences also offer opportunities for learning and adapting best practices across borders.","children":[],"payload":{"lines":"201,202"}},{"content":"<strong>Global Influence</strong>: The EU AI Act's comprehensive approach to AI regulation may influence other jurisdictions to adopt similar or complementary measures, promoting a global regulatory environment that balances innovation with ethical considerations and human rights protection.","children":[],"payload":{"lines":"202,204"}}],"payload":{"lines":"199,200"}}],"payload":{"lines":"192,193"}},{"content":"Challenges and Criticisms","children":[{"content":"Innovation vs Regulation Balance","children":[{"content":"<strong>Striking the Right Balance</strong>: One of the most significant challenges is ensuring that the Act fosters a safe and trustworthy AI environment without stifling innovation. Critics argue that overly stringent regulations could hamper the EU's competitiveness in the global AI market.","children":[{"content":"<strong>Concerns for Startups and SMEs</strong>: Small and medium-sized enterprises (SMEs) and startups might find it particularly challenging to meet the regulatory requirements due to limited resources, potentially hindering diversity and innovation in the AI ecosystem.","children":[],"payload":{"lines":"208,209"}},{"content":"<strong>Dynamic Nature of AI</strong>: The fast-evolving nature of AI technology means regulations need to be flexible and adaptive, without becoming outdated or excessively burdensome.","children":[],"payload":{"lines":"209,211"}}],"payload":{"lines":"207,211"}}],"payload":{"lines":"206,207"}},{"content":"Enforcement and Compliance","children":[{"content":"<strong>Complexity of Enforcement</strong>: The global and decentralized nature of AI development and deployment poses significant challenges to enforcing the Act. There's a concern about the capacity of national supervisory authorities to effectively monitor and enforce compliance, especially given the technical complexity of AI systems.","children":[{"content":"<strong>International Cooperation</strong>: Effective enforcement may require unprecedented levels of international cooperation, raising questions about jurisdiction, data sharing, and cross-border legal challenges.","children":[],"payload":{"lines":"213,214"}},{"content":"<strong>Consistency Across EU</strong>: Ensuring consistent application of the AI Act across all EU member states is another challenge, given the varying levels of resources and expertise available in different countries.","children":[],"payload":{"lines":"214,216"}}],"payload":{"lines":"212,216"}}],"payload":{"lines":"211,212"}},{"content":"Technical Feasibility","children":[{"content":"<strong>Assessment and Compliance</strong>: The requirement for high-risk AI systems to undergo rigorous testing and conformity assessments before deployment is seen as crucial for safety and ethics. However, there are concerns about the technical feasibility of such assessments, especially for cutting-edge AI technologies.","children":[{"content":"<strong>Rapid Technological Advances</strong>: The pace of innovation in AI might outstrip the ability of regulatory frameworks to adapt, leading to situations where the Act's requirements are either too vague or overly prescriptive for new AI developments.","children":[],"payload":{"lines":"218,219"}},{"content":"<strong>Interpretation of Standards</strong>: The technical standards and requirements laid out in the Act need to be precise enough to be enforceable but flexible enough to accommodate future technological advancements. Finding this balance is a significant challenge.","children":[],"payload":{"lines":"219,221"}}],"payload":{"lines":"217,221"}}],"payload":{"lines":"216,217"}}],"payload":{"lines":"204,205"}},{"content":"Future Prospects","children":[{"content":"Amendments and Revisions","children":[{"content":"<strong>Adaptive Framework</strong>: The EU AI Act includes mechanisms for regular review and amendment to ensure that it remains relevant and effective in the face of technological progress and emerging challenges.","children":[{"content":"<strong>Review Process</strong>: Scheduled reviews of the Act will assess its impact, effectiveness, and any need for adjustments based on the development of AI technologies and their applications.","children":[],"payload":{"lines":"225,226"}},{"content":"<strong>Stakeholder Engagement</strong>: Amendments and revisions will involve consultation with a broad range of stakeholders, including AI developers, users, academic experts, and civil society, to ensure a comprehensive understanding of the evolving AI ecosystem.","children":[],"payload":{"lines":"226,228"}}],"payload":{"lines":"224,228"}}],"payload":{"lines":"223,224"}},{"content":"Expansion of Scope","children":[{"content":"<strong>Emerging Technologies and Applications</strong>: As new forms of AI and applications emerge, the Act may need to expand its scope to address risks and challenges not previously contemplated or adequately covered by the original legislation.","children":[{"content":"<strong>Risk Reclassification</strong>: Ongoing assessment of AI technologies could lead to reclassification of certain AI systems as higher or lower risk, necessitating changes in regulatory requirements and oversight.","children":[],"payload":{"lines":"230,231"}},{"content":"<strong>Global Developments</strong>: International trends in AI governance and technological advancements may influence the EU to adjust the Act's scope, either to align with global standards or to address unique challenges arising within the EU context.","children":[],"payload":{"lines":"231,233"}}],"payload":{"lines":"229,233"}}],"payload":{"lines":"228,229"}}],"payload":{"lines":"221,222"}},{"content":"Considerations for Compliance","children":[{"content":"Determining AI System Status","children":[{"content":"\n<p data-lines=\"238,239\"><strong>Do we have an \"AI system\"?</strong></p>","children":[{"content":"<strong>Machine-based system</strong>: It is designed to operate with varying levels of autonomy.","children":[],"payload":{"lines":"239,240"}},{"content":"<strong>Adaptiveness</strong>: It may exhibit adaptiveness after deployment.","children":[],"payload":{"lines":"240,241"}},{"content":"<strong>Objectives</strong>: For explicit or implicit objectives, infers from the input it receives how to generate outputs such as predictions, content, recommendations, or decisions.","children":[],"payload":{"lines":"241,242"}},{"content":"<strong>Influence</strong>: The output can influence physical or virtual environments.","children":[],"payload":{"lines":"242,243"}},{"content":"<strong>General Purpose AI</strong>: AI models with significant generality, able to perform many tasks, can be integrated into many applications.","children":[],"payload":{"lines":"243,245"}}],"payload":{"lines":"238,245"}},{"content":"\n<p data-lines=\"245,246\"><strong>Which role do we have?</strong></p>","children":[{"content":"<strong>Provider</strong>: We develop an AI system or a general-purpose AI model (or have this done by a 3rd party) and have it placed on the EU market or put into service in the EU.","children":[],"payload":{"lines":"246,247"}},{"content":"<strong>Deployer</strong>: We use an AI system under our authority (except for personal non-professional activities).","children":[],"payload":{"lines":"247,248"}},{"content":"<strong>Importer</strong>: We are established or located in the EU and place on the EU market an AI system bearing the name of someone outside the EU.","children":[],"payload":{"lines":"248,249"}},{"content":"<strong>Distributor</strong>: We make an AI system available on the EU market but are neither the provider nor the importer.","children":[],"payload":{"lines":"249,250"}},{"content":"<strong>Product Manufacturer</strong>: We place on the market or put into service in the EU an AI system with our product under our own name.","children":[],"payload":{"lines":"250,252"}}],"payload":{"lines":"245,252"}},{"content":"\n<p data-lines=\"252,253\"><strong>Are we within the scope of the Act?</strong></p>","children":[{"content":"<strong>Provider</strong>: We place on the market or put into service AI systems in the EU, place on the EU market general-purpose AI models, or the output of the AI system is to be used in the EU.","children":[],"payload":{"lines":"253,254"}},{"content":"<strong>Deployer</strong>: We are established or located in the EU, or the output of the AI system is used in the EU.","children":[],"payload":{"lines":"254,255"}},{"content":"<strong>Importer, Distributor, Product Manufacturer</strong>: As defined above.","children":[],"payload":{"lines":"255,257"}}],"payload":{"lines":"252,257"}},{"content":"\n<p data-lines=\"257,258\"><strong>Will our use case be a \"high-risk\" AI system?</strong></p>","children":[{"content":"<strong>Safety Component</strong>: The AI system is a safety component of a product or a product itself that under existing EU law has to undergo a third-party conformity assessment.","children":[],"payload":{"lines":"258,259"}},{"content":"<strong>Biometric Identification</strong>: Remote biometric identification beyond mere authentication.","children":[],"payload":{"lines":"259,260"}},{"content":"<strong>Critical Infrastructure</strong>: AI used as a safety component in the management and operation of critical infrastructure.","children":[],"payload":{"lines":"260,261"}},{"content":"<strong>Education and Vocational Training</strong>: AI determining access, admission, or assignment in education, or evaluating learning outcomes.","children":[],"payload":{"lines":"261,262"}},{"content":"<strong>Employment</strong>: AI for recruitment, selection, or decisions affecting terms of employment.","children":[],"payload":{"lines":"262,263"}},{"content":"<strong>Public Assistance</strong>: AI for evaluating eligibility for public assistance benefits.","children":[],"payload":{"lines":"263,264"}},{"content":"<strong>Creditworthiness</strong>: AI for evaluating a person's creditworthiness.","children":[],"payload":{"lines":"264,265"}},{"content":"<strong>Emergency Services</strong>: AI for classifying emergency calls or dispatching emergency services.","children":[],"payload":{"lines":"265,266"}},{"content":"<strong>Insurance</strong>: AI for risk assessments and pricing of life or health insurance.","children":[],"payload":{"lines":"266,267"}},{"content":"<strong>Law Enforcement</strong>: AI for assessing the risk of offending or re-offending, assessing personality traits, or profiling in criminal investigations.","children":[],"payload":{"lines":"267,268"}},{"content":"<strong>Migration and Border Control</strong>: AI for assessing risks posed by persons entering the EU, examining applications for asylum, and identifying persons.","children":[],"payload":{"lines":"268,269"}},{"content":"<strong>Judicial Authority</strong>: AI assisting judicial authorities in interpreting facts and the law.","children":[],"payload":{"lines":"269,270"}},{"content":"<strong>Elections</strong>: AI influencing the outcome of an election or referendum.","children":[],"payload":{"lines":"270,272"}}],"payload":{"lines":"257,272"}},{"content":"\n<p data-lines=\"272,273\"><strong>Other cases that require deployers to act</strong></p>","children":[{"content":"<strong>Emotion and Intent Detection</strong>: AI systems inferring or detecting emotions and intents or biometric categorization.","children":[],"payload":{"lines":"273,274"}},{"content":"<strong>Deep Fakes</strong>: AI systems generating \"deep fakes\" must disclose the content has been artificially generated or manipulated.","children":[],"payload":{"lines":"274,275"}},{"content":"<strong>AI-generated Text</strong>: AI systems generating or manipulating text published for informing the public must disclose the text has been artificially generated or manipulated.","children":[],"payload":{"lines":"275,276"}},{"content":"<strong>AI Literacy</strong>: Obligation to train/ensure AI literacy of those dealing with AI systems.","children":[],"payload":{"lines":"276,278"}}],"payload":{"lines":"272,278"}},{"content":"\n<p data-lines=\"278,279\"><strong>Is our use case prohibited under the Act?</strong></p>","children":[{"content":"<strong>Manipulative Techniques</strong>: Use of subliminal, purposefully manipulative, or deceptive techniques to materially distort behavior.","children":[],"payload":{"lines":"279,280"}},{"content":"<strong>Exploiting Vulnerabilities</strong>: Exploiting vulnerabilities of persons due to age, disability, or specific social/economic situations.","children":[],"payload":{"lines":"280,281"}},{"content":"<strong>Biometric Categorization</strong>: Biometric categorization to deduce a person's race, political opinion, trade union membership, etc.","children":[],"payload":{"lines":"281,282"}},{"content":"<strong>Social Scoring</strong>: Evaluation of persons based on social behavior leading to detrimental treatment.","children":[],"payload":{"lines":"282,283"}},{"content":"<strong>Real-time Biometric Identification</strong>: Real-time biometric identification in public spaces for law enforcement purposes.","children":[],"payload":{"lines":"283,284"}},{"content":"<strong>Criminal Offense Prediction</strong>: Profiling for predicting the risk of committing criminal offenses.","children":[],"payload":{"lines":"284,285"}},{"content":"<strong>Facial Recognition Databases</strong>: Creation or expansion of facial recognition databases based on untargeted scraping.","children":[],"payload":{"lines":"285,286"}},{"content":"<strong>Emotion Inference in Workplaces</strong>: Inferring emotions in workplace areas or educational institutions except for medical or safety reasons.","children":[],"payload":{"lines":"286,288"}}],"payload":{"lines":"278,288"}}],"payload":{"lines":"235,236"}}],"payload":{"lines":"233,234"}},{"content":"Conclusion","children":[{"content":"Ensure you seek expert legal advice to navigate the EU AI Act and its implications for your AI system. For more details, explore the AI Act online and utilize resources like the Generative AI Risk Assessment and Generative AI Risk Check.","children":[],"payload":{"lines":"289,290"}}],"payload":{"lines":"288,289"}}],"payload":{"lines":"0,1"}},null)</script>
</body>
</html>
